{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled8.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prathmesh4321/CS231/blob/master/pointnet_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "On9hkuxBx7Rs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "outputId": "698a0570-affb-462f-9f29-dcfdb8a5c353"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PL_Y8GEdx-As",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "84cfc020-31c2-4f20-aa62-19aa755ddd4c"
      },
      "source": [
        "!wget https://github.com/charlesq34/pointnet/archive/master.zip -P '/content/drive/My Drive/pointnet_train' \n",
        "  "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-08-30 09:38:04--  https://github.com/charlesq34/pointnet/archive/master.zip\n",
            "Resolving github.com (github.com)... 140.82.113.4\n",
            "Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://codeload.github.com/charlesq34/pointnet/zip/master [following]\n",
            "--2019-08-30 09:38:05--  https://codeload.github.com/charlesq34/pointnet/zip/master\n",
            "Resolving codeload.github.com (codeload.github.com)... 140.82.113.10\n",
            "Connecting to codeload.github.com (codeload.github.com)|140.82.113.10|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/zip]\n",
            "Saving to: ‘/content/drive/My Drive/pointnet_train/master.zip.1’\n",
            "\n",
            "master.zip.1            [ <=>                ]  33.00K  91.4KB/s               "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-2-8dcfade58757>\", line 1, in <module>\n",
            "    get_ipython().system(\"wget https://github.com/charlesq34/pointnet/archive/master.zip -P '/content/drive/My Drive/pointnet_train' \")\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/google/colab/_shell.py\", line 84, in system\n",
            "    output = _system_commands._system_compat(self, *args, **kwargs)  # pylint:disable=protected-access\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/google/colab/_system_commands.py\", line 438, in _system_compat\n",
            "    shell.var_expand(cmd, depth=2), clear_streamed_output=False)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/google/colab/_system_commands.py\", line 195, in _run_command\n",
            "    return _monitor_process(parent_pty, epoll, p, cmd, update_stdin_widget)\n",
            "  File \"/usr/lib/python3.6/contextlib.py\", line 88, in __exit__\n",
            "    next(self.gen)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/google/colab/_system_commands.py\", line 355, in _display_stdin_widget\n",
            "    _message.blocking_request(*hide_args, parent=shell.parent_header)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/google/colab/_message.py\", line 171, in blocking_request\n",
            "    return read_reply_from_input(request_id, timeout_sec)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/google/colab/_message.py\", line 101, in read_reply_from_input\n",
            "    time.sleep(0.025)\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 1823, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/ultratb.py\", line 1132, in get_records\n",
            "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/ultratb.py\", line 313, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/ultratb.py\", line 358, in _fixed_getinnerframes\n",
            "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
            "  File \"/usr/lib/python3.6/inspect.py\", line 1490, in getinnerframes\n",
            "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
            "  File \"/usr/lib/python3.6/inspect.py\", line 1448, in getframeinfo\n",
            "    filename = getsourcefile(frame) or getfile(frame)\n",
            "  File \"/usr/lib/python3.6/inspect.py\", line 696, in getsourcefile\n",
            "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
            "  File \"/usr/lib/python3.6/inspect.py\", line 742, in getmodule\n",
            "    os.path.realpath(f)] = module.__name__\n",
            "  File \"/usr/lib/python3.6/posixpath.py\", line 395, in realpath\n",
            "    path, ok = _joinrealpath(filename[:0], filename, {})\n",
            "  File \"/usr/lib/python3.6/posixpath.py\", line 429, in _joinrealpath\n",
            "    if not islink(newpath):\n",
            "  File \"/usr/lib/python3.6/posixpath.py\", line 171, in islink\n",
            "    st = os.lstat(path)\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42Ao1c6oyf7H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 797
        },
        "outputId": "abb143b9-1094-4a94-ff67-a7f7870a60a5"
      },
      "source": [
        "!unzip \"/content/drive/My Drive/pointnet_train/master.zip\" -d \"/content/drive/My Drive/pointnet_train/\""
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/drive/My Drive/pointnet_train/master.zip\n",
            "d64d2398e55b24f69e95ecb549ff7d4581ffc21e\n",
            "   creating: /content/drive/My Drive/pointnet_train/pointnet-master/\n",
            "  inflating: /content/drive/My Drive/pointnet_train/pointnet-master/LICENSE  \n",
            "  inflating: /content/drive/My Drive/pointnet_train/pointnet-master/README.md  \n",
            "   creating: /content/drive/My Drive/pointnet_train/pointnet-master/doc/\n",
            "  inflating: /content/drive/My Drive/pointnet_train/pointnet-master/doc/teaser.png  \n",
            "  inflating: /content/drive/My Drive/pointnet_train/pointnet-master/evaluate.py  \n",
            "   creating: /content/drive/My Drive/pointnet_train/pointnet-master/models/\n",
            "  inflating: /content/drive/My Drive/pointnet_train/pointnet-master/models/pointnet_cls.py  \n",
            "  inflating: /content/drive/My Drive/pointnet_train/pointnet-master/models/pointnet_cls_basic.py  \n",
            "  inflating: /content/drive/My Drive/pointnet_train/pointnet-master/models/pointnet_seg.py  \n",
            "  inflating: /content/drive/My Drive/pointnet_train/pointnet-master/models/transform_nets.py  \n",
            "   creating: /content/drive/My Drive/pointnet_train/pointnet-master/part_seg/\n",
            "  inflating: /content/drive/My Drive/pointnet_train/pointnet-master/part_seg/download_data.sh  \n",
            "  inflating: /content/drive/My Drive/pointnet_train/pointnet-master/part_seg/pointnet_part_seg.py  \n",
            "  inflating: /content/drive/My Drive/pointnet_train/pointnet-master/part_seg/test.py  \n",
            "  inflating: /content/drive/My Drive/pointnet_train/pointnet-master/part_seg/testing_ply_file_list.txt  \n",
            "  inflating: /content/drive/My Drive/pointnet_train/pointnet-master/part_seg/train.py  \n",
            "  inflating: /content/drive/My Drive/pointnet_train/pointnet-master/provider.py  \n",
            "   creating: /content/drive/My Drive/pointnet_train/pointnet-master/sem_seg/\n",
            "  inflating: /content/drive/My Drive/pointnet_train/pointnet-master/sem_seg/README.md  \n",
            "  inflating: /content/drive/My Drive/pointnet_train/pointnet-master/sem_seg/batch_inference.py  \n",
            "  inflating: /content/drive/My Drive/pointnet_train/pointnet-master/sem_seg/collect_indoor3d_data.py  \n",
            "  inflating: /content/drive/My Drive/pointnet_train/pointnet-master/sem_seg/download_data.sh  \n",
            "  inflating: /content/drive/My Drive/pointnet_train/pointnet-master/sem_seg/eval_iou_accuracy.py  \n",
            "  inflating: /content/drive/My Drive/pointnet_train/pointnet-master/sem_seg/gen_indoor3d_h5.py  \n",
            "  inflating: /content/drive/My Drive/pointnet_train/pointnet-master/sem_seg/indoor3d_util.py  \n",
            "   creating: /content/drive/My Drive/pointnet_train/pointnet-master/sem_seg/meta/\n",
            "  inflating: /content/drive/My Drive/pointnet_train/pointnet-master/sem_seg/meta/all_data_label.txt  \n",
            "  inflating: /content/drive/My Drive/pointnet_train/pointnet-master/sem_seg/meta/anno_paths.txt  \n",
            "  inflating: /content/drive/My Drive/pointnet_train/pointnet-master/sem_seg/meta/area6_data_label.txt  \n",
            "  inflating: /content/drive/My Drive/pointnet_train/pointnet-master/sem_seg/meta/class_names.txt  \n",
            "  inflating: /content/drive/My Drive/pointnet_train/pointnet-master/sem_seg/model.py  \n",
            "  inflating: /content/drive/My Drive/pointnet_train/pointnet-master/sem_seg/train.py  \n",
            "  inflating: /content/drive/My Drive/pointnet_train/pointnet-master/train.py  \n",
            "   creating: /content/drive/My Drive/pointnet_train/pointnet-master/utils/\n",
            "  inflating: /content/drive/My Drive/pointnet_train/pointnet-master/utils/data_prep_util.py  \n",
            "  inflating: /content/drive/My Drive/pointnet_train/pointnet-master/utils/eulerangles.py  \n",
            "  inflating: /content/drive/My Drive/pointnet_train/pointnet-master/utils/pc_util.py  \n",
            "  inflating: /content/drive/My Drive/pointnet_train/pointnet-master/utils/plyfile.py  \n",
            "  inflating: /content/drive/My Drive/pointnet_train/pointnet-master/utils/tf_util.py  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8wI45ChSzZHv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import argparse\n",
        "import math\n",
        "import h5py\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import socket\n",
        "import importlib\n",
        "import os\n",
        "import sys\n",
        "BASE_DIR = os.path.dirname('/content/drive/My Drive/pointnet_train/pointnet-master/')\n",
        "print(BASE_DIR )\n",
        "sys.path.append(BASE_DIR)\n",
        "sys.path.append(os.path.join(BASE_DIR, 'models'))\n",
        "sys.path.append(os.path.join(BASE_DIR, 'utils'))\n",
        "import provider\n",
        "import tf_util\n",
        "\n",
        "\"\"\"parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--gpu', type=int, default=0, help='GPU to use [default: GPU 0]')\n",
        "parser.add_argument('--model', default='pointnet_cls', help='Model name: pointnet_cls or pointnet_cls_basic [default: pointnet_cls]')\n",
        "parser.add_argument('--log_dir', default='log', help='Log dir [default: log]')\n",
        "parser.add_argument('--num_point', type=int, default=1024, help='Point Number [256/512/1024/2048] [default: 1024]')\n",
        "parser.add_argument('--max_epoch', type=int, default=250, help='Epoch to run [default: 250]')\n",
        "parser.add_argument('--batch_size', type=int, default=32, help='Batch Size during training [default: 32]')\n",
        "parser.add_argument('--learning_rate', type=float, default=0.001, help='Initial learning rate [default: 0.001]')\n",
        "parser.add_argument('--momentum', type=float, default=0.9, help='Initial learning rate [default: 0.9]')\n",
        "parser.add_argument('--optimizer', default='adam', help='adam or momentum [default: adam]')\n",
        "parser.add_argument('--decay_step', type=int, default=200000, help='Decay step for lr decay [default: 200000]')\n",
        "parser.add_argument('--decay_rate', type=float, default=0.7, help='Decay rate for lr decay [default: 0.8]')\n",
        "FLAGS = parser.parse_args()\"\"\"\n",
        "\n",
        "model='pointnet_cls_basic'\n",
        "BATCH_SIZE = 32\n",
        "NUM_POINT = 1024\n",
        "MAX_EPOCH = 250\n",
        "BASE_LEARNING_RATE = 0.01\n",
        "GPU_INDEX = 0\n",
        "MOMENTUM = 0.9\n",
        "OPTIMIZER = 'adam'\n",
        "DECAY_STEP = 200000\n",
        "DECAY_RATE = 0.7\n",
        "\n",
        "MODEL = importlib.import_module('pointnet_cls_basic') # import network module\n",
        "MODEL_FILE = os.path.join(BASE_DIR, 'models', model+'.py')\n",
        "LOG_DIR = '/content/drive/My Drive/pointnet_train/pointnet-master/log/'\n",
        "if not os.path.exists(LOG_DIR): os.mkdir(LOG_DIR)\n",
        "os.system('cp %s %s' % (MODEL_FILE, LOG_DIR)) # bkp of model def\n",
        "os.system('cp train.py %s' % (LOG_DIR)) # bkp of train procedure\n",
        "LOG_FOUT = open(os.path.join(LOG_DIR, 'log_train.txt'), 'w')\n",
        "#LOG_FOUT.write(str(FLAGS)+'\\n')\n",
        "\n",
        "MAX_NUM_POINT = 2048\n",
        "NUM_CLASSES = 40\n",
        "\n",
        "BN_INIT_DECAY = 0.5\n",
        "BN_DECAY_DECAY_RATE = 0.5\n",
        "BN_DECAY_DECAY_STEP = float(DECAY_STEP)\n",
        "BN_DECAY_CLIP = 0.99\n",
        "\n",
        "HOSTNAME = socket.gethostname()\n",
        "os.chdir('/content/drive/My Drive/pointnet/PointnetEnhanced-master/pointnet/')\n",
        "# ModelNet40 official train/test split\n",
        "TRAIN_FILES = provider.getDataFiles('/content/drive/My Drive/pointnet/PointnetEnhanced-master/pointnet/data/modelnet40_ply_hdf5_2048/train_files.txt')\n",
        "TEST_FILES = provider.getDataFiles('/content/drive/My Drive/pointnet/PointnetEnhanced-master/pointnet/data/modelnet40_ply_hdf5_2048/test_files.txt')\n",
        "\n",
        "def log_string(out_str):\n",
        "    LOG_FOUT.write(out_str+'\\n')\n",
        "    LOG_FOUT.flush()\n",
        "    print(out_str)\n",
        "\n",
        "\n",
        "def get_learning_rate(batch):\n",
        "    learning_rate = tf.train.exponential_decay(\n",
        "                        BASE_LEARNING_RATE,  # Base learning rate.\n",
        "                        batch * BATCH_SIZE,  # Current index into the dataset.\n",
        "                        DECAY_STEP,          # Decay step.\n",
        "                        DECAY_RATE,          # Decay rate.\n",
        "                        staircase=True)\n",
        "    learning_rate = tf.maximum(learning_rate, 0.00001) # CLIP THE LEARNING RATE!\n",
        "    return learning_rate        \n",
        "\n",
        "def get_bn_decay(batch):\n",
        "    bn_momentum = tf.train.exponential_decay(\n",
        "                      BN_INIT_DECAY,\n",
        "                      batch*BATCH_SIZE,\n",
        "                      BN_DECAY_DECAY_STEP,\n",
        "                      BN_DECAY_DECAY_RATE,\n",
        "                      staircase=True)\n",
        "    bn_decay = tf.minimum(BN_DECAY_CLIP, 1 - bn_momentum)\n",
        "    return bn_decay\n",
        "\n",
        "def train():\n",
        "    with tf.Graph().as_default():\n",
        "        with tf.device('/gpu:'+str(GPU_INDEX)):\n",
        "            pointclouds_pl, labels_pl = MODEL.placeholder_inputs(BATCH_SIZE, NUM_POINT)\n",
        "            is_training_pl = tf.placeholder(tf.bool, shape=())\n",
        "            print(is_training_pl)\n",
        "            \n",
        "            # Note the global_step=batch parameter to minimize. \n",
        "            # That tells the optimizer to helpfully increment the 'batch' parameter for you every time it trains.\n",
        "            batch = tf.Variable(0)\n",
        "            bn_decay = get_bn_decay(batch)\n",
        "            tf.summary.scalar('bn_decay', bn_decay)\n",
        "\n",
        "            # Get model and loss \n",
        "            pred, end_points = MODEL.get_model(pointclouds_pl, is_training_pl, bn_decay=bn_decay)\n",
        "            loss = MODEL.get_loss(pred, labels_pl, end_points)\n",
        "            tf.summary.scalar('loss', loss)\n",
        "\n",
        "            correct = tf.equal(tf.argmax(pred, 1), tf.to_int64(labels_pl))\n",
        "            accuracy = tf.reduce_sum(tf.cast(correct, tf.float32)) / float(BATCH_SIZE)\n",
        "            tf.summary.scalar('accuracy', accuracy)\n",
        "\n",
        "            # Get training operator\n",
        "            learning_rate = get_learning_rate(batch)\n",
        "            tf.summary.scalar('learning_rate', learning_rate)\n",
        "            if OPTIMIZER == 'momentum':\n",
        "                optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=MOMENTUM)\n",
        "            elif OPTIMIZER == 'adam':\n",
        "                optimizer = tf.train.AdamOptimizer(learning_rate)\n",
        "            train_op = optimizer.minimize(loss, global_step=batch)\n",
        "            \n",
        "            # Add ops to save and restore all the variables.\n",
        "            saver = tf.train.Saver()\n",
        "            \n",
        "        # Create a session\n",
        "        config = tf.ConfigProto()\n",
        "        config.gpu_options.allow_growth = True\n",
        "        config.allow_soft_placement = True\n",
        "        config.log_device_placement = False\n",
        "        sess = tf.Session(config=config)\n",
        "\n",
        "        # Add summary writers\n",
        "        #merged = tf.merge_all_summaries()\n",
        "        merged = tf.summary.merge_all()\n",
        "        train_writer = tf.summary.FileWriter(os.path.join(LOG_DIR, 'train'),\n",
        "                                  sess.graph)\n",
        "        test_writer = tf.summary.FileWriter(os.path.join(LOG_DIR, 'test'))\n",
        "\n",
        "        # Init variables\n",
        "        init = tf.global_variables_initializer()\n",
        "        # To fix the bug introduced in TF 0.12.1 as in\n",
        "        # http://stackoverflow.com/questions/41543774/invalidargumenterror-for-tensor-bool-tensorflow-0-12-1\n",
        "        #sess.run(init)\n",
        "        sess.run(init, {is_training_pl: True})\n",
        "\n",
        "        ops = {'pointclouds_pl': pointclouds_pl,\n",
        "               'labels_pl': labels_pl,\n",
        "               'is_training_pl': is_training_pl,\n",
        "               'pred': pred,\n",
        "               'loss': loss,\n",
        "               'train_op': train_op,\n",
        "               'merged': merged,\n",
        "               'step': batch}\n",
        "\n",
        "        for epoch in range(MAX_EPOCH):\n",
        "            log_string('**** EPOCH %03d ****' % (epoch))\n",
        "            sys.stdout.flush()\n",
        "             \n",
        "            train_one_epoch(sess, ops, train_writer)\n",
        "            eval_one_epoch(sess, ops, test_writer)\n",
        "            \n",
        "            # Save the variables to disk.\n",
        "            if epoch % 10 == 0:\n",
        "                save_path = saver.save(sess, os.path.join(LOG_DIR, \"model.ckpt\"))\n",
        "                log_string(\"Model saved in file: %s\" % save_path)\n",
        "\n",
        "\n",
        "\n",
        "def train_one_epoch(sess, ops, train_writer):\n",
        "    \"\"\" ops: dict mapping from string to tf ops \"\"\"\n",
        "    is_training = True\n",
        "    \n",
        "    # Shuffle train files\n",
        "    train_file_idxs = np.arange(0, len(TRAIN_FILES))\n",
        "    np.random.shuffle(train_file_idxs)\n",
        "    \n",
        "    for fn in range(len(TRAIN_FILES)):\n",
        "        log_string('----' + str(fn) + '-----')\n",
        "        current_data, current_label = provider.loadDataFile(TRAIN_FILES[train_file_idxs[fn]])\n",
        "        current_data = current_data[:,0:NUM_POINT,:]\n",
        "        current_data, current_label, _ = provider.shuffle_data(current_data, np.squeeze(current_label))            \n",
        "        current_label = np.squeeze(current_label)\n",
        "        \n",
        "        file_size = current_data.shape[0]\n",
        "        num_batches = file_size // BATCH_SIZE\n",
        "        \n",
        "        total_correct = 0\n",
        "        total_seen = 0\n",
        "        loss_sum = 0\n",
        "       \n",
        "        for batch_idx in range(num_batches):\n",
        "            start_idx = batch_idx * BATCH_SIZE\n",
        "            end_idx = (batch_idx+1) * BATCH_SIZE\n",
        "            \n",
        "            # Augment batched point clouds by rotation and jittering\n",
        "            rotated_data = provider.rotate_point_cloud(current_data[start_idx:end_idx, :, :])\n",
        "            jittered_data = provider.jitter_point_cloud(rotated_data)\n",
        "            feed_dict = {ops['pointclouds_pl']: jittered_data,\n",
        "                         ops['labels_pl']: current_label[start_idx:end_idx],\n",
        "                         ops['is_training_pl']: is_training,}\n",
        "            summary, step, _, loss_val, pred_val = sess.run([ops['merged'], ops['step'],\n",
        "                ops['train_op'], ops['loss'], ops['pred']], feed_dict=feed_dict)\n",
        "            train_writer.add_summary(summary, step)\n",
        "            pred_val = np.argmax(pred_val, 1)\n",
        "            correct = np.sum(pred_val == current_label[start_idx:end_idx])\n",
        "            total_correct += correct\n",
        "            total_seen += BATCH_SIZE\n",
        "            loss_sum += loss_val\n",
        "        \n",
        "        log_string('mean loss: %f' % (loss_sum / float(num_batches)))\n",
        "        log_string('accuracy: %f' % (total_correct / float(total_seen)))\n",
        "\n",
        "        \n",
        "def eval_one_epoch(sess, ops, test_writer):\n",
        "    \"\"\" ops: dict mapping from string to tf ops \"\"\"\n",
        "    is_training = False\n",
        "    total_correct = 0\n",
        "    total_seen = 0\n",
        "    loss_sum = 0\n",
        "    total_seen_class = [0 for _ in range(NUM_CLASSES)]\n",
        "    total_correct_class = [0 for _ in range(NUM_CLASSES)]\n",
        "    \n",
        "    for fn in range(len(TEST_FILES)):\n",
        "        log_string('----' + str(fn) + '-----')\n",
        "        current_data, current_label = provider.loadDataFile(TEST_FILES[fn])\n",
        "        current_data = current_data[:,0:NUM_POINT,:]\n",
        "        current_label = np.squeeze(current_label)\n",
        "        \n",
        "        file_size = current_data.shape[0]\n",
        "        num_batches = file_size // BATCH_SIZE\n",
        "        \n",
        "        for batch_idx in range(num_batches):\n",
        "            start_idx = batch_idx * BATCH_SIZE\n",
        "            end_idx = (batch_idx+1) * BATCH_SIZE\n",
        "\n",
        "            feed_dict = {ops['pointclouds_pl']: current_data[start_idx:end_idx, :, :],\n",
        "                         ops['labels_pl']: current_label[start_idx:end_idx],\n",
        "                         ops['is_training_pl']: is_training}\n",
        "            summary, step, loss_val, pred_val = sess.run([ops['merged'], ops['step'],\n",
        "                ops['loss'], ops['pred']], feed_dict=feed_dict)\n",
        "            pred_val = np.argmax(pred_val, 1)\n",
        "            correct = np.sum(pred_val == current_label[start_idx:end_idx])\n",
        "            total_correct += correct\n",
        "            total_seen += BATCH_SIZE\n",
        "            loss_sum += (loss_val*BATCH_SIZE)\n",
        "            for i in range(start_idx, end_idx):\n",
        "                l = current_label[i]\n",
        "                total_seen_class[l] += 1\n",
        "                total_correct_class[l] += (pred_val[i-start_idx] == l)\n",
        "            \n",
        "    log_string('eval mean loss: %f' % (loss_sum / float(total_seen)))\n",
        "    log_string('eval accuracy: %f'% (total_correct / float(total_seen)))\n",
        "    log_string('eval avg class acc: %f' % (np.mean(np.array(total_correct_class)/np.array(total_seen_class,dtype=np.float))))\n",
        "         \n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train()\n",
        "LOG_FOUT.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SpHkU6jg1E_s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}